{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen \n",
    "import re \n",
    "from time import sleep \n",
    "from collections import Counter \n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_cleaner(soup_obj):\n",
    "    '''\n",
    "    Inputs: a BeautifulSoup object to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    if len(soup_obj) == 0:\n",
    "        soup_obj = BeautifulSoup(page, 'html5lib')\n",
    "    \n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    text = soup_obj.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines()) \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) \n",
    "    text = ''.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_url(role, location):\n",
    "    location = location.replace(' ','+')\n",
    "    role = role.replace(' ','+')\n",
    "    return 'https://www.indeed.com/jobs?q='+role+'&l='+location+'&sort=date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_urls(search_url):\n",
    "    base_url = 'https://www.indeed.com'\n",
    "    search = urlopen(search_url)\n",
    "    soup = BeautifulSoup(search)\n",
    "    num_jobs_area = soup.find(id = 'searchCount').text\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area)\n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2])\n",
    "    num_pages = total_num_jobs/10 \n",
    "\n",
    "    job_url = [] \n",
    "    for i in range(1,int(num_pages+1)): \n",
    "        start_num = str(i*10)\n",
    "        current_page = ''.join([search_url, '&start=', start_num])\n",
    "        \n",
    "        html_page = urlopen(current_page).read()\n",
    "        page_obj = BeautifulSoup(html_page)\n",
    "        job_link_area = page_obj.find(id = 'resultsCol')\n",
    "        for a in [link.find_all('a') for link in job_link_area.find_all('div') if link.get('class') == ['title']]:\n",
    "            job_url.append(base_url+a[0].get('href'))\n",
    "    return job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(jobs):\n",
    "    try:\n",
    "        data = []\n",
    "        for job_url in jobs:\n",
    "            job = {}\n",
    "            page = urlopen(job_url)\n",
    "            bs_obj = BeautifulSoup(page)\n",
    "            #print(job_url)\n",
    "            #print('----------------------------------------------------------------------------------------------------')\n",
    "            if '-' not in bs_obj.find('title').text:\n",
    "                continue\n",
    "            try:\n",
    "                job['company'] = bs_obj.find('div', class_ = 'icl-u-lg-mr--sm icl-u-xs-mr--xs').text\n",
    "            except:\n",
    "                job['company'] = bs_obj.find('div', class_ = 'icl-u-xs-mt--xs icl-u-textColor--secondary').text\n",
    "            title = bs_obj.find('title').text\n",
    "            job['position'] = title.split(' - ')[-3]\n",
    "            if ',' in title.split(' - ')[-2]:\n",
    "                job['city'] = title.split(' - ')[-2].split(', ')[0]\n",
    "                job['state'] = title.split(' - ')[-2].split(', ')[1]\n",
    "            else:\n",
    "                job['city'] = '-'\n",
    "                job['state'] =  '-'           \n",
    "            job['jd'] = page_cleaner(bs_obj)\n",
    "            job['url'] = job_url\n",
    "            data.append(job)\n",
    "        return data\n",
    "    except:\n",
    "        return 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(pos, loc):\n",
    "    url = prep_url(pos, loc)\n",
    "    jobs = get_job_urls(url)\n",
    "    data = scraper(jobs)\n",
    "    jobs_df = pd.DataFrame(data)\n",
    "    jobs_df['state'] = jobs_df['state'].str.split().str[0]\n",
    "    return jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine_learning = prep_df('machine learning', 'USA')\n",
    "#data_analyst = prep_df('data analyst', 'USA')\n",
    "#data_scientist = prep_df('data scientist', 'USA')\n",
    "#data_engineer = prep_df('data engineer', 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning.to_csv('machine_learning_jobs.csv',index=False)\n",
    "data_analyst.to_csv('data_analyst.csv',index=False)\n",
    "data_scientist.to_csv('data_scientist.csv',index=False)\n",
    "data_engineer.to_csv('data_engineer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_state(df):\n",
    "    print(df.groupby('state')['jd'].count().reset_index().sort_values('jd',ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = jobs_df['jd'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector = cv.fit_transform(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "#df_test=pd.read_json(\"data/stackoverflow-test.json\",lines=True)\n",
    "#df_test['text'] = df_test['title'] + df_test['body']\n",
    "df_test['jd'] =df_test['jd'].apply(lambda x:pre_process(x))\n",
    " \n",
    "# get test docs into a list\n",
    "docs_test=df_test['jd'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[0]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "doc = ' '.join(docs_test)\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "temp=extract_topn_from_vector(feature_names,sorted_items,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c29269e3eaf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
